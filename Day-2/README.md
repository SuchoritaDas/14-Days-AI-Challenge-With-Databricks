# Day 02 â€“ Apache Spark Fundamentals
On Day 2 of the Databricks 14-Day AI Challenge, I worked with real e-commerce data using Apache Spark and PySpark inside Databricks.
## What I did
Uploaded a sample e-commerce CSV file
Loaded the data into a Spark DataFrame
Explored the schema using printSchema()
Used select, filter, groupBy, and orderBy
Found event counts and top-selling brands
Exported query results
## Key analysis performed
Counted different event types (view, cart, purchase)
Filtered products with price greater than 100
Identified top brands based on user activity
Sorted and limited results for business-friendly insights
## What I learned
How Spark reads large CSV files
How DataFrames work in PySpark
How to summarize raw data into useful metrics
## Key takeaway
Apache Spark makes it easy to process large datasets and turn raw events into meaningful insights.
